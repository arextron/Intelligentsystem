# ðŸ“š Multi-Agent Chatbot with Reinforcement Learning - Documentation

## ðŸ”§ Project Overview
This project implements a modular multi-agent chatbot system with:
- Contextual routing (General, AI, Admission agents)
- Feedback collection and logging
- Reinforcement Learning-style reranking using a reward model
- Streamlit dashboard for evaluation and analytics

It uses `LangChain`, `Ollama` (Llama3), and integrates with `FastAPI`, `scikit-learn`, and `Streamlit`.

---

## ðŸ“ Code Structure

```
project-root/
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ general_agent.py       # Handles general queries with Wikipedia context
â”‚   â”œâ”€â”€ ai_agent.py            # Handles AI/ML/NLP related queries
â”‚   â””â”€â”€ admission_agent.py     # Handles Concordia CS admissions queries
â”‚
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ vector_store.py        # In-memory context store per user (chat history)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ external_api.py        # Wikipedia API wrapper
â”‚   â””â”€â”€ reward_model.py        # RL reward model based on sklearn or LLM
â”‚
â”œâ”€â”€ main.py                    # FastAPI entry point with routing and RL logic
â”œâ”€â”€ dashboard.py               # Streamlit dashboard for feedback analytics
â”œâ”€â”€ index.html                 # Chatbot frontend UI (vanilla HTML+JS)
â”œâ”€â”€ chat_logs.csv              # Log of chats (prompt, response, agent, rating)
â””â”€â”€ reward_model_sklearn.pkl   # Trained reward model for reranking
```

---

## ðŸ¤– Agents
Each agent uses LangChain pipelines:
- Constructs prompt + context
- Calls Ollama Llama3 model
- Stores interactions in `VectorStore`

All agents expose:
```python
def handle_query(query: str, user_id: str) -> str
```
And now support RL via:
```python
def generate_candidates(query: str, user_id: str, n=3) -> List[str]
```

---

## ðŸ” main.py - Core Routing
- Accepts `/chat` POST with `user_input` and `user_id`
- Detects intent: AI, Admission, or General
- Retrieves conversation context
- Uses `.generate_candidates()` to sample N completions
- Ranks completions using reward model
- Returns best-scoring response

Feedback is logged via `/feedback` POST. These logs are stored in `chat_logs.csv`.

---

## ðŸ§  Reward Model
Located at `utils/reward_model.py`.
- Can use:
  - LLM-based score (via prompt to Ollama)
  - or sklearn model (TF-IDF + Ridge from logs)
- Scoring function:
```python
def score(prompt, response) -> float  # 0.0 to 1.0
```
- Selection:
```python
def select_best(prompt, candidates) -> str
```

Trained reward model is saved as `reward_model_sklearn.pkl` and loaded for real-time scoring.

---

## ðŸ“Š Streamlit Dashboard
Launch with:
```bash
streamlit run dashboard.py
```
Features:
- Chat summary (users, sessions)
- Rating distribution
- Agent-wise accuracy
- RL-based auto-evaluation from LLM
- Logs export

It uses the `chat_logs.csv` file generated by `main.py` to visualize chatbot performance and behavior.

---

## ðŸ§ª RLHF Training (Optional)
To retrain or improve the reward model:
1. Collect logs from `chat_logs.csv`
2. Compute rewards using user feedback and/or LLM evaluator
3. Train a regression model (e.g. TF-IDF + Ridge)
4. Save model as `reward_model_sklearn.pkl`
5. Plug into `RewardModel` for reranking

To run training manually:
```bash
python train_reward_model.py
```
Ensure this script reads `chat_logs.csv`, trains the regressor, and saves `reward_model_sklearn.pkl`

---

## ðŸ›  Future Improvements
- Fine-tune Llama3 via QLoRA on logs
- Add safety/toxicity filtering
- Boost diversity using distinct-n or entropy
- Memory persistence with Redis or ChromaDB
- Real-time analytics and retraining pipeline

---

## ðŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Start FastAPI Backend
```bash
uvicorn main:app --reload
```

### 3. Start Frontend UI
Open a terminal and run:
```bash
python -m http.server 9000
```
Then visit: [http://localhost:9000/index.html](http://localhost:9000/index.html)

### 4. Launch Analytics Dashboard
```bash
streamlit run dashboard.py
```

### 5. Train or Reuse Reward Model
If training for the first time:
```bash
python utils/reward_model.py
```
Make sure it creates `reward_model_sklearn.pkl` and places it in the root or correct `utils` path.

That's it â€” you're now running a **fully self-improving AI assistant**! ðŸ§ 

